{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.4980, 0.1255, 0.9725, 0.3961, 0.8275, 0.6392, 0.5686, 0.5255,\n          0.0941, 0.2588, 0.0392, 0.5490, 0.4980, 0.6392, 0.7882, 0.7647,\n          0.3961, 0.8784, 0.7725, 0.1059, 0.4118, 0.8863, 0.1255, 0.3059,\n          0.1137, 0.8863, 0.7922, 0.7569, 0.9961, 0.0314, 0.3294, 0.9882]]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "pixel_values = torch.randint(0, 255, (1, 1, 32))\n",
    "pixel_values = pixel_values / 255\n",
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0039, -0.7490,  0.9451, -0.2078,  0.6549,  0.2784,  0.1373,\n           0.0510, -0.8118, -0.4824, -0.9216,  0.0980, -0.0039,  0.2784,\n           0.5765,  0.5294, -0.2078,  0.7569,  0.5451, -0.7882, -0.1765,\n           0.7725, -0.7490, -0.3882, -0.7725,  0.7725,  0.5843,  0.5137,\n           0.9922, -0.9373, -0.3412,  0.9765]]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalize(mean=0.5, std=0.5)\n",
    "norm_pixel = normalizer(pixel_values)\n",
    "norm_pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-0.0039, -0.7490,  0.9451, -0.2078,  0.6549,  0.2784,  0.1373,\n            0.0510],\n          [-0.8118, -0.4824, -0.9216,  0.0980, -0.0039,  0.2784,  0.5765,\n            0.5294],\n          [-0.2078,  0.7569,  0.5451, -0.7882, -0.1765,  0.7725, -0.7490,\n           -0.3882],\n          [-0.7725,  0.7725,  0.5843,  0.5137,  0.9922, -0.9373, -0.3412,\n            0.9765]]]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_pixel = norm_pixel.reshape(1, 1, 4, 8)\n",
    "norm_pixel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[111., 156.,  64.,  45., 168., 192., 130., 114.]]]),\n torch.Size([1, 1, 8]))"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "pixel_values = [random.randint(0, 255) for i in range(8)]\n",
    "pixel_values = torch.as_tensor(pixel_values, dtype=torch.float).view(1, 1, -1)\n",
    "pixel_values, pixel_values.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.1294,  0.2235, -0.4980, -0.6471,  0.3176,  0.5059,  0.0196,\n           -0.1059]]]),\n torch.Size([1, 1, 8]))"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "normalize = Normalize(mean=0.5, std=0.5)\n",
    "norm_pixel_values = normalize(pixel_values / 255)\n",
    "norm_pixel_values, norm_pixel_values.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.1294,  0.2235, -0.4980, -0.6471,  0.3176,  0.5059,  0.0196,\n           -0.1059,  0.0000,  0.0000]]]),\n torch.Size([1, 1, 10]))"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_pixel_values = torch.nn.functional.pad(norm_pixel_values, (0, 2, 0, 0))\n",
    "pad_pixel_values, pad_pixel_values.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}