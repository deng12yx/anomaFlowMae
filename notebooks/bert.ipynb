{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(max_position_embeddings=8)\n",
    "word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[96, 15, 27,  4, 76, 86],\n         [43, 65, 51, 20, 52, 56]]),\n torch.Size([2, 6]),\n 6)"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.randint(0, 100, (2, 6))\n",
    "input_shape = input_ids.size()\n",
    "seq_length = input_shape[1]\n",
    "input_ids, input_shape, seq_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1018, -0.0436, -0.2491,  ..., -1.5319,  0.1579, -0.3988],\n",
      "         [-1.0177, -0.4927, -0.6918,  ..., -0.2705, -1.0987,  1.4907],\n",
      "         [ 0.2334, -0.2801,  0.2963,  ...,  1.0481,  0.2028, -0.7454],\n",
      "         [-0.8064,  1.0662, -0.0228,  ...,  1.0990,  1.8425,  0.3876],\n",
      "         [-2.9163, -0.2970, -0.1240,  ..., -1.3806,  1.8677,  0.8364],\n",
      "         [-0.7654, -1.2803, -0.4555,  ..., -0.5180,  0.5751,  0.8081]],\n",
      "\n",
      "        [[ 2.8438, -1.1507, -0.1720,  ...,  0.7628,  1.2780,  2.2947],\n",
      "         [ 0.7472, -1.8190,  0.3092,  ...,  0.0908, -0.3583,  2.8664],\n",
      "         [ 1.5884,  0.5041, -0.5914,  ...,  0.9934, -1.9225,  0.3489],\n",
      "         [ 0.3765,  0.4048,  0.3921,  ..., -0.6353, -0.9512, -0.0740],\n",
      "         [-0.4986,  1.1924,  1.2099,  ..., -0.3461, -1.1524,  1.6119],\n",
      "         [ 0.5000,  0.8541, -0.9925,  ..., -0.5671, -1.0413, -0.3625]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, torch.Size([2, 6, 768]))"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds = word_embeddings(input_ids)\n",
    "print(inputs_embeds), inputs_embeds.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n",
    "position_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3, 4, 5]])"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = position_ids[:, : seq_length]\n",
    "position_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0620,  0.5503, -0.3741,  ..., -0.4613,  0.8329, -0.7874],\n",
      "         [-0.2094,  0.8066, -1.0739,  ..., -1.0512,  1.3952,  0.4911],\n",
      "         [ 1.4946, -0.2448, -0.0059,  ..., -0.5901,  0.7230,  1.5968],\n",
      "         [-0.1138, -1.2172,  0.2797,  ...,  0.8967, -0.3172, -0.4115],\n",
      "         [-0.1085, -0.9684, -0.7007,  ...,  0.1512,  0.7826,  0.2522],\n",
      "         [ 1.3071, -0.8792, -1.1434,  ..., -0.0795, -1.0724, -1.2303]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, torch.Size([1, 6, 768]))"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embedding = position_embeddings(position_ids)\n",
    "print(position_embedding), position_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 0, 0, 0, 0]])"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids = torch.zeros(position_ids.size(), dtype=torch.long)\n",
    "token_type_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0]]),\n tensor([[0, 0, 0, 0, 0, 0]]))"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffered_token_type_ids = token_type_ids[:, :seq_length]\n",
    "buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "token_type_ids = buffered_token_type_ids_expanded\n",
    "token_type_ids, buffered_token_type_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845]],\n",
      "\n",
      "        [[-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845],\n",
      "         [-0.0810,  0.0851, -0.4403,  ..., -0.6212,  0.0225,  0.2845]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(None, torch.Size([2, 6, 768]))"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_embedding = token_type_embeddings(token_type_ids)\n",
    "print(token_type_embedding), token_type_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.8792,  0.5919, -1.0635,  ..., -2.6145,  1.0133, -0.9017],\n          [-1.3081,  0.3991, -2.2060,  ..., -1.9430,  0.3189,  2.2663],\n          [ 1.6470, -0.4397, -0.1499,  ..., -0.1632,  0.9483,  1.1359],\n          [-1.0012, -0.0658, -0.1833,  ...,  1.3745,  1.5478,  0.2607],\n          [-3.1058, -1.1802, -1.2649,  ..., -1.8506,  2.6727,  1.3731],\n          [ 0.4607, -2.0743, -2.0393,  ..., -1.2188, -0.4748, -0.1377]],\n \n         [[ 3.8248, -0.5153, -0.9863,  ..., -0.3197,  2.1333,  1.7918],\n          [ 0.4568, -0.9272, -1.2051,  ..., -1.5816,  1.0593,  3.6420],\n          [ 3.0020,  0.3445, -1.0376,  ..., -0.2179, -1.1771,  2.2302],\n          [ 0.1817, -0.7273,  0.2316,  ..., -0.3597, -1.2459, -0.2010],\n          [-0.6881,  0.3092,  0.0689,  ..., -0.8161, -0.3473,  2.1486],\n          [ 1.7261,  0.0600, -2.5762,  ..., -1.2679, -2.0913, -1.3083]]],\n        grad_fn=<AddBackward0>),\n torch.Size([2, 6, 768]))"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = inputs_embeds + token_type_embedding + position_embedding\n",
    "embeddings, embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.5604,  0.3906, -0.5873,  ..., -1.5035,  0.6396, -0.4917],\n          [-0.7050,  0.3186, -1.2434,  ..., -1.0857,  0.2705,  1.4382],\n          [ 0.9863, -0.2433, -0.0726,  ..., -0.0804,  0.5746,  0.6851],\n          [-0.6119, -0.0720, -0.1398,  ...,  0.7594,  0.8594,  0.1165],\n          [-1.7621, -0.6545, -0.7032,  ..., -1.0401,  1.5619,  0.8143],\n          [ 0.2480, -1.2471, -1.2264,  ..., -0.7425, -0.3037, -0.1049]],\n \n         [[ 2.2425, -0.2528, -0.5236,  ..., -0.1403,  1.2700,  1.0737],\n          [ 0.2940, -0.5253, -0.6898,  ..., -0.9127,  0.6507,  2.1797],\n          [ 1.7575,  0.2001, -0.6099,  ..., -0.1295, -0.6916,  1.3052],\n          [ 0.0708, -0.4751,  0.1007,  ..., -0.2544, -0.7866, -0.1591],\n          [-0.4311,  0.1662,  0.0223,  ..., -0.5078, -0.2270,  1.2679],\n          [ 1.0954,  0.0811, -1.5238,  ..., -0.7273, -1.2285, -0.7519]]],\n        grad_fn=<NativeLayerNormBackward0>),\n torch.Size([2, 6, 768]))"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "norm_embeddings = LayerNorm(embeddings)\n",
    "norm_embeddings, norm_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.6227,  0.4340, -0.6525,  ..., -1.6706,  0.0000, -0.5463],\n          [-0.7834,  0.3540, -1.3816,  ..., -1.2063,  0.3006,  1.5980],\n          [ 1.0959, -0.2704, -0.0806,  ..., -0.0893,  0.6384,  0.7612],\n          [-0.6799, -0.0800, -0.0000,  ...,  0.8438,  0.9549,  0.1294],\n          [-1.9579, -0.7272, -0.7814,  ..., -1.1557,  1.7354,  0.9047],\n          [ 0.2756, -1.3857, -1.3627,  ..., -0.8250, -0.3375, -0.1165]],\n \n         [[ 2.4916, -0.2809, -0.5818,  ..., -0.1559,  1.4111,  1.1929],\n          [ 0.3266, -0.5837, -0.7665,  ..., -0.0000,  0.0000,  0.0000],\n          [ 1.9528,  0.2223, -0.6777,  ..., -0.1439, -0.7685,  1.4502],\n          [ 0.0786, -0.5279,  0.1119,  ..., -0.2827, -0.8740, -0.1768],\n          [-0.4790,  0.1847,  0.0248,  ..., -0.0000, -0.2522,  1.4087],\n          [ 1.2171,  0.0901, -1.6931,  ..., -0.8081, -1.3651, -0.8354]]],\n        grad_fn=<MulBackward0>),\n torch.Size([2, 6, 768]))"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "out_embeddings = dropout(norm_embeddings)\n",
    "out_embeddings, out_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.ones(2, 8)\n",
    "attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1.]]],\n\n\n        [[[1., 1., 1., 1., 1., 1., 1., 1.]]]])"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = attention_mask[:, None, None, :]\n",
    "extended_attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102],\n        [ 101, 1996, 3007, 1997,  102,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer([\"The capital of France is [MASK].\", \"The capital of\"], return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"input_ids\": [[101, 1996, 3007, 1997, 2605, 2003, 103, 1012, 102]]\n",
    "}\n",
    "special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "batch[\"input_ids\"], batch[\"labels\"] = numpy_mask_tokens(\n",
    "    batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15]])"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "probability_matrix = np.full((1, 9), 0.15)\n",
    "probability_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76931/3007215653.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  special_tokens_mask = np.array([[1, 0, 0, 0, 0, 1, 0, 0, 1]], dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ True, False, False, False, False,  True, False, False,  True]])"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_mask = np.array([[1, 0, 0, 0, 0, 1, 0, 0, 1]], dtype=np.bool)\n",
    "special_tokens_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.  , 0.15, 0.15, 0.15, 0.15, 0.  , 0.15, 0.15, 0.  ]])"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrix[special_tokens_mask] = 0\n",
    "probability_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76931/2726413271.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[False, False, False, False, False, False,  True, False, False]])"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(np.bool)\n",
    "masked_indices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[    0,     0,     0, -1000]]]])"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.as_tensor([\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "    [0.1, 0.1, 0.1, 0.1],\n",
    "])\n",
    "attention_mask = torch.as_tensor([[0, 0, 0, -1000]])\n",
    "extended_attention_mask = attention_mask[:, None, None, :]\n",
    "extended_attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 1.0000e-01,  1.0000e-01,  1.0000e-01, -9.9990e+02],\n          [ 1.0000e-01,  1.0000e-01,  1.0000e-01, -9.9990e+02],\n          [ 1.0000e-01,  1.0000e-01,  1.0000e-01, -9.9990e+02],\n          [ 1.0000e-01,  1.0000e-01,  1.0000e-01, -9.9990e+02]]]])"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attention_scores = attention_scores + extended_attention_mask\n",
    "final_attention_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0.3333, 0.3333, 0.3333, 0.0000],\n          [0.3333, 0.3333, 0.3333, 0.0000],\n          [0.3333, 0.3333, 0.3333, 0.0000],\n          [0.3333, 0.3333, 0.3333, 0.0000]]]])"
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs = nn.functional.softmax(final_attention_scores, dim=-1)\n",
    "attention_probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4, 8])"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = torch.rand((2, 4, 8), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "batch_size, seq_length, dim = sequence.shape\n",
    "mask_ratio = 0.25\n",
    "sequence.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4])"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = torch.rand(batch_size, seq_length, device=\"cuda\")\n",
    "noise.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 4]),\n tensor([[3, 1, 2, 0],\n         [2, 3, 1, 0]], device='cuda:0'))"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_shuffle = torch.argsort(noise, dim=1)\n",
    "ids_shuffle.shape, ids_shuffle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[3, 1, 2],\n         [2, 3, 1]], device='cuda:0'),\n torch.Size([2, 3]))"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_keep = int(seq_length * (1 - mask_ratio))\n",
    "ids_keep = ids_shuffle[:, :len_keep]\n",
    "ids_keep, ids_keep.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "data": {
      "text/plain": "(3,\n tensor([[[0.4018, 0.7857, 0.0249, 0.1331, 0.1187, 0.9118, 0.3672, 0.7224],\n          [0.7811, 0.7241, 0.9312, 0.2073, 0.8186, 0.9522, 0.2588, 0.8637],\n          [0.7325, 0.7208, 0.3008, 0.2485, 0.1720, 0.9731, 0.7791, 0.8823]],\n \n         [[0.4355, 0.2511, 0.7570, 0.3515, 0.3261, 0.3251, 0.1136, 0.0240],\n          [0.0359, 0.6223, 0.4285, 0.9332, 0.0145, 0.5448, 0.8405, 0.0043],\n          [0.1547, 0.1175, 0.1563, 0.5687, 0.1850, 0.6222, 0.7419, 0.8627]]],\n        device='cuda:0'),\n torch.Size([2, 3, 8]),\n tensor([[[0.6399, 0.9781, 0.5286, 0.6497, 0.1382, 0.3921, 0.4539, 0.7756],\n          [0.7811, 0.7241, 0.9312, 0.2073, 0.8186, 0.9522, 0.2588, 0.8637],\n          [0.7325, 0.7208, 0.3008, 0.2485, 0.1720, 0.9731, 0.7791, 0.8823],\n          [0.4018, 0.7857, 0.0249, 0.1331, 0.1187, 0.9118, 0.3672, 0.7224]],\n \n         [[0.1781, 0.2082, 0.8804, 0.8122, 0.0999, 0.5284, 0.3716, 0.7465],\n          [0.1547, 0.1175, 0.1563, 0.5687, 0.1850, 0.6222, 0.7419, 0.8627],\n          [0.4355, 0.2511, 0.7570, 0.3515, 0.3261, 0.3251, 0.1136, 0.0240],\n          [0.0359, 0.6223, 0.4285, 0.9332, 0.0145, 0.5448, 0.8405, 0.0043]]],\n        device='cuda:0'))"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_masked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n",
    "len_keep, sequence_masked, sequence_masked.shape, sequence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 4]),\n tensor([[3, 1, 2, 0],\n         [3, 2, 0, 1]], device='cuda:0'))"
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "ids_restore.shape, ids_restore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 0., 0., 1.],\n         [0., 0., 0., 1.]], device='cuda:0'),\n torch.Size([2, 4]))"
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones([batch_size, seq_length], device=sequence.device)\n",
    "mask[:, :len_keep] = 0\n",
    "mask, mask.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 0., 0., 0.],\n         [1., 0., 0., 0.]], device='cuda:0'),\n torch.Size([2, 4]))"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_masked = torch.gather(mask, dim=1, index=ids_restore)\n",
    "mask_masked, mask_masked.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 1, 1, 0],\n         [1, 1, 1, 1]], device='cuda:0'),\n torch.Size([2, 4]))"
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.as_tensor(\n",
    "    [\n",
    "        [1] * 3 + [0] * (seq_length - 3),\n",
    "        [1] * 4 + [0] * (seq_length - 4)\n",
    "    ]\n",
    "    , device=\"cuda\"\n",
    ")\n",
    "attention_mask, attention_mask.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 1],\n        [1, 1, 1]], device='cuda:0')"
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_masked = torch.gather(attention_mask, dim=1, index=ids_keep)\n",
    "attention_mask_masked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0, 1, 1]]],\n\n\n        [[[1, 1, 1]]]], device='cuda:0')"
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = attention_mask_masked[:, None, None, :]\n",
    "extended_attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-3.4028e+38, -0.0000e+00, -0.0000e+00]]],\n\n\n        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00]]]], device='cuda:0')"
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(torch.float32).min\n",
    "extended_attention_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_masked = torch.gather(mask, dim=1, index=ids_restore)\n",
    "mask_masked, mask_masked.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
