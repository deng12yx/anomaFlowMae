{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(ViTMAEConfig {\n   \"attention_probs_dropout_prob\": 0.0,\n   \"decoder_hidden_size\": 512,\n   \"decoder_intermediate_size\": 2048,\n   \"decoder_num_attention_heads\": 16,\n   \"decoder_num_hidden_layers\": 8,\n   \"hidden_act\": \"gelu\",\n   \"hidden_dropout_prob\": 0.0,\n   \"hidden_size\": 768,\n   \"image_size\": 224,\n   \"initializer_range\": 0.02,\n   \"intermediate_size\": 3072,\n   \"layer_norm_eps\": 1e-12,\n   \"mask_ratio\": 0.75,\n   \"model_type\": \"vit_mae\",\n   \"norm_pix_loss\": false,\n   \"num_attention_heads\": 12,\n   \"num_channels\": 3,\n   \"num_hidden_layers\": 12,\n   \"patch_size\": 16,\n   \"qkv_bias\": true,\n   \"transformers_version\": \"4.22.1\"\n },\n ViTMAEPatchEmbeddings(\n   (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n ),\n ViTMAEEmbeddings(\n   (patch_embeddings): ViTMAEPatchEmbeddings(\n     (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n   )\n ))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.configuration_vit_mae import ViTMAEConfig\n",
    "from model.mae import ViTMAEEmbeddings, ViTMAEPatchEmbeddings\n",
    "\n",
    "config = ViTMAEConfig()\n",
    "patch_embeddings = ViTMAEPatchEmbeddings(config)\n",
    "vit_embeddings = ViTMAEEmbeddings(config)\n",
    "config, patch_embeddings, vit_embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[0.4951, 0.0721, 0.4896,  ..., 0.2390, 0.9004, 0.4541],\n           [0.7855, 0.5213, 0.3566,  ..., 0.8841, 0.0404, 0.3115],\n           [0.6544, 0.8796, 0.6899,  ..., 0.8787, 0.1300, 0.7642],\n           ...,\n           [0.4336, 0.1213, 0.7397,  ..., 0.0684, 0.6291, 0.8053],\n           [0.6662, 0.8739, 0.1029,  ..., 0.8111, 0.3116, 0.9347],\n           [0.9309, 0.7031, 0.0873,  ..., 0.1985, 0.1508, 0.3664]],\n \n          [[0.7349, 0.9654, 0.3532,  ..., 0.8188, 0.3163, 0.7806],\n           [0.4397, 0.2318, 0.7431,  ..., 0.9633, 0.8252, 0.9041],\n           [0.1053, 0.0518, 0.4995,  ..., 0.3392, 0.3501, 0.3687],\n           ...,\n           [0.6691, 0.0453, 0.1278,  ..., 0.6316, 0.6591, 0.2662],\n           [0.9109, 0.7459, 0.5275,  ..., 0.8578, 0.6595, 0.7037],\n           [0.2074, 0.3899, 0.6594,  ..., 0.3186, 0.3859, 0.8327]],\n \n          [[0.2771, 0.5120, 0.7551,  ..., 0.4928, 0.5669, 0.4894],\n           [0.1395, 0.7947, 0.4307,  ..., 0.7243, 0.5125, 0.1282],\n           [0.3015, 0.5739, 0.2133,  ..., 0.7591, 0.9349, 0.1289],\n           ...,\n           [0.9960, 0.6189, 0.0188,  ..., 0.2474, 0.4445, 0.1920],\n           [0.6691, 0.0178, 0.2604,  ..., 0.3769, 0.4374, 0.5295],\n           [0.6079, 0.4815, 0.8788,  ..., 0.5977, 0.5827, 0.6122]]],\n \n \n         [[[0.8270, 0.1096, 0.0693,  ..., 0.1140, 0.8318, 0.2858],\n           [0.7254, 0.4544, 0.1430,  ..., 0.3127, 0.3055, 0.0664],\n           [0.2151, 0.9959, 0.7584,  ..., 0.9518, 0.0289, 0.0069],\n           ...,\n           [0.3700, 0.1115, 0.3901,  ..., 0.3530, 0.4954, 0.6169],\n           [0.0513, 0.6604, 0.2191,  ..., 0.3726, 0.2889, 0.9100],\n           [0.4631, 0.7592, 0.9844,  ..., 0.3096, 0.3617, 0.3624]],\n \n          [[0.5939, 0.2937, 0.5042,  ..., 0.4621, 0.1254, 0.9711],\n           [0.5087, 0.7455, 0.2670,  ..., 0.1291, 0.9194, 0.6112],\n           [0.7936, 0.6144, 0.8023,  ..., 0.0145, 0.0190, 0.2531],\n           ...,\n           [0.6096, 0.2262, 0.2728,  ..., 0.2268, 0.5569, 0.1794],\n           [0.4160, 0.1982, 0.3743,  ..., 0.3627, 0.6010, 0.2814],\n           [0.1816, 0.1479, 0.0376,  ..., 0.8469, 0.0979, 0.9080]],\n \n          [[0.4395, 0.5210, 0.4387,  ..., 0.6518, 0.7812, 0.6016],\n           [0.7853, 0.1505, 0.2126,  ..., 0.8136, 0.1838, 0.6163],\n           [0.0810, 0.0624, 0.8331,  ..., 0.3834, 0.6901, 0.8801],\n           ...,\n           [0.6969, 0.6794, 0.5456,  ..., 0.9197, 0.2210, 0.1776],\n           [0.7229, 0.2391, 0.7815,  ..., 0.3254, 0.9483, 0.6502],\n           [0.5336, 0.3227, 0.1956,  ..., 0.8762, 0.0811, 0.2150]]]]),\n torch.Size([2, 3, 224, 224]))"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pixel_values = torch.rand(2, 3, 224, 224)\n",
    "pixel_values, pixel_values.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.1511,  0.3573, -0.0502,  ..., -0.2963,  0.0760, -0.4688],\n          [ 0.1600,  0.3279,  0.1730,  ..., -0.1618,  0.2018, -0.2984],\n          [ 0.2183,  0.2686,  0.3596,  ..., -0.0807,  0.0930, -0.4436],\n          ...,\n          [-0.0231, -0.0600,  0.0582,  ..., -0.2219,  0.4858, -0.3078],\n          [ 0.1643,  0.2149,  0.3243,  ...,  0.0296,  0.4256, -0.0761],\n          [ 0.2710,  0.1624,  0.3388,  ..., -0.1865,  0.3348, -0.3663]],\n \n         [[ 0.3880,  0.4705,  0.2075,  ..., -0.2487,  0.3270, -0.4334],\n          [ 0.1031,  0.1022,  0.2230,  ..., -0.0991,  0.1923, -0.4863],\n          [ 0.1071,  0.3450, -0.0761,  ..., -0.2565,  0.4488, -0.7837],\n          ...,\n          [ 0.3444,  0.3786, -0.0654,  ..., -0.0384,  0.3723, -0.3631],\n          [ 0.1927,  0.5171, -0.1595,  ...,  0.0655,  0.5558, -0.6155],\n          [ 0.2298,  0.3001,  0.2935,  ..., -0.0123,  0.1376, -0.2856]]],\n        grad_fn=<TransposeBackward0>),\n torch.Size([2, 196, 768]))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_patch = patch_embeddings(pixel_values)\n",
    "emb_patch, emb_patch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [ 0.8415,  0.8153,  0.7886,  ...,  1.0000,  1.0000,  1.0000],\n          ...,\n          [-1.0000, -0.8724, -0.5387,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.5366, -0.9037, -0.9956,  ...,  1.0000,  1.0000,  1.0000],\n          [ 0.4202, -0.1744, -0.6858,  ...,  1.0000,  1.0000,  1.0000]]]),\n torch.Size([1, 197, 768]))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_embeddings.position_embeddings, vit_embeddings.position_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.1511,  0.3573, -0.0502,  ...,  0.7037,  1.0760,  0.5312],\n          [ 1.0015,  1.1432,  0.9616,  ...,  0.8382,  1.2018,  0.7016],\n          [ 1.1276,  1.2128,  1.3295,  ...,  0.9193,  1.0930,  0.5564],\n          ...,\n          [-1.0231, -0.9323, -0.4805,  ...,  0.7781,  1.4858,  0.6922],\n          [-0.3722, -0.6888, -0.6713,  ...,  1.0296,  1.4256,  0.9239],\n          [ 0.6912, -0.0120, -0.3470,  ...,  0.8135,  1.3348,  0.6337]],\n \n         [[ 0.3880,  0.4705,  0.2075,  ...,  0.7513,  1.3270,  0.5666],\n          [ 0.9445,  0.9175,  1.0116,  ...,  0.9009,  1.1923,  0.5137],\n          [ 1.0164,  1.2892,  0.8938,  ...,  0.7435,  1.4488,  0.2163],\n          ...,\n          [-0.6556, -0.4938, -0.6042,  ...,  0.9616,  1.3723,  0.6369],\n          [-0.3438, -0.3866, -1.1552,  ...,  1.0655,  1.5558,  0.3845],\n          [ 0.6499,  0.1257, -0.3923,  ...,  0.9877,  1.1376,  0.7144]]],\n        grad_fn=<AddBackward0>),\n torch.Size([2, 196, 768]))"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_pos = emb_patch + vit_embeddings.position_embeddings[:, 1:, :]\n",
    "emb_pos, emb_pos.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.5278,  0.2685, -0.6473,  ...,  0.8113,  1.0937,  0.7003],\n          [-0.5090, -0.2033, -0.3931,  ...,  0.9461,  1.2617,  0.8079],\n          [ 0.5182,  0.7603,  0.9714,  ...,  0.9044,  1.1647,  0.5977],\n          ...,\n          [ 0.7222, -0.2479, -0.4524,  ...,  0.8263,  1.2958,  0.6242],\n          [-0.7043, -0.5573, -0.9992,  ...,  0.7991,  0.9773,  0.7021],\n          [-0.7664, -0.6398, -0.3093,  ...,  0.9467,  1.2165,  0.4633]],\n \n         [[-0.2689, -0.0183, -0.5378,  ...,  0.6198,  1.1503,  0.7082],\n          [-0.4604, -0.2877, -0.8647,  ...,  0.8965,  0.9679,  0.4046],\n          [ 1.3429,  0.9516,  1.3404,  ...,  1.0460,  1.1793,  0.9541],\n          ...,\n          [-0.6682, -0.2119, -0.2083,  ...,  0.6230,  1.5158,  0.8301],\n          [-0.0754,  0.2375,  0.7344,  ...,  0.9857,  1.5763,  0.4417],\n          [ 0.5742,  1.3351,  1.1841,  ...,  1.1348,  1.2267,  0.6618]]],\n        grad_fn=<GatherBackward0>),\n torch.Size([2, 49, 768]),\n tensor([[1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n          1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n          0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n          1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n          0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n          1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.],\n         [0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n          1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n          1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n          1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n          0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n          0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.]]),\n torch.Size([2, 196]),\n tensor([[194, 140,  13,  16,  68,  59,  63,  11, 150,  55, 115,  29,  15,  93,\n           70,  77,  49,  98,  80, 116,  21,  78, 183, 125,  92, 153,  44, 172,\n          163,  42, 164,  95,  84, 121,  86, 111, 147, 145, 175, 173, 124,  76,\n          100, 178,  38,  99,  83, 120,  18,  26, 142,  10, 109,  51, 108,  87,\n          148,  56, 192, 104,  96,  61, 174, 127, 133, 195, 161, 176,  89, 182,\n          158, 114,  75,   4,  69,  82,  81, 185,  54, 156, 134, 184, 186,   8,\n          143, 149, 132, 189,   1,  58,  41, 131,  64,  79, 117,   3,  90,  46,\n          188,  24, 128, 168, 180, 154,  50,  52, 144, 170, 190, 165,  47,  17,\n          102, 159,  71, 122, 187,  35, 123, 129,  31, 101, 177, 141, 162,   0,\n          126, 106,   9,  32,  28,  45,  94,   5,  25,  73,  62, 160, 193, 112,\n           85, 169,  72,  43,  20,   7,  22,  66,  67, 181, 157,  57, 179,  53,\n          155, 191,  36, 103,  23,  97, 166,  88, 105, 136, 130, 152, 119,  91,\n          139,  27, 171,  39, 167,  65,  14, 151,  30, 135,  60,  48, 137, 118,\n          113,  74,  33,  34,  12, 107,   6,  37,  40,   2, 110, 146, 138,  19],\n         [ 43, 185,  65, 153, 141,  84, 103,  60,  92,  48, 180,  55,  95, 177,\n           80, 192, 165, 158, 168,  74,  87, 159,  58, 120,  34, 123,  30, 127,\n           52, 119, 132,  73, 134,   8,   0, 124, 107, 125,  94, 178, 179, 187,\n           45,  97, 156,  15, 148,  56,  49, 111, 193,  86,  91, 146, 145,  67,\n           12,  66,  22, 170, 195, 166,  63,  21, 122, 126, 137,  76,  93, 157,\n           81,  96, 150, 140,  46, 186,  26,  10,   6, 171, 121, 160,  61,  75,\n           53,  64,  33, 101, 169,  20, 106, 183,  44,   4, 194,  17, 184,  72,\n          164, 149,  37,  32,  70,  59,  18, 118,   9,  28, 143, 182, 139,  90,\n           68, 113, 155, 116, 100,  62,  50,  35, 163,  16,  82, 176, 138, 142,\n          181, 152, 115,  77,  57,  54, 144,  78, 104,   5,   3, 133, 135,  99,\n           25,   2,   7,  31, 114,  89, 191, 174,  29, 167,  13, 112,  23,  98,\n          109, 110,  51,  42,  24,  14,  19, 129,  40, 162,  69,  88, 128, 172,\n           79, 117, 175, 108, 188,  38, 189,  11,  39, 147, 154, 173,   1, 130,\n           85,  27,  83, 105,  41,  71, 161, 131, 102, 136,  47, 151,  36, 190]]),\n torch.Size([2, 196]))"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings, mask, ids_restore = vit_embeddings.random_masking(emb_pos)\n",
    "embeddings, embeddings.shape, mask, mask.shape, ids_restore, ids_restore.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[[ 1.0774e-02,  7.7184e-03,  5.8833e-03,  8.9845e-03, -3.4352e-02,\n            3.4248e-03,  4.7736e-03,  1.3227e-02,  1.8124e-02, -1.5146e-02,\n           -2.5704e-02,  1.8151e-02, -1.5896e-02, -3.8172e-02,  1.8153e-02,\n           -1.9877e-02, -9.5269e-04, -4.5868e-02, -3.2151e-03,  1.2454e-02,\n           -4.6562e-02, -2.1301e-04, -3.2736e-02, -1.6554e-02, -2.0955e-02,\n           -4.7700e-03,  4.0389e-02,  2.0755e-02,  3.0682e-02,  1.7040e-02,\n            8.8829e-03,  8.8651e-03,  1.8762e-02,  1.7758e-03,  2.7143e-03,\n            1.2448e-02,  5.2147e-02, -1.2137e-02,  3.9249e-03, -5.1513e-02,\n            1.9513e-02,  8.2119e-03,  7.0421e-03, -3.1023e-05, -1.2054e-02,\n            3.2577e-02,  2.4956e-02,  1.1123e-02,  2.2424e-03,  2.4977e-03,\n           -9.4353e-03,  8.4229e-03, -2.2655e-02, -1.4874e-02,  1.3114e-02,\n            1.1049e-02, -1.7435e-02, -3.5369e-02,  2.3251e-03,  1.9796e-02,\n            7.8293e-03,  2.6074e-03, -2.9100e-04, -8.9573e-03,  8.1006e-03,\n           -3.2777e-02,  1.2541e-02,  4.5902e-03, -4.5868e-02, -1.8093e-02,\n           -4.8141e-02,  2.5668e-02,  1.3822e-02, -3.4809e-03, -5.2264e-03,\n            8.5803e-03, -1.3684e-02, -2.1483e-02, -1.1502e-02,  4.4336e-02,\n           -1.4205e-02, -1.0597e-02, -1.2623e-02, -2.0844e-03,  2.9332e-03,\n            6.8332e-03,  6.1783e-03,  7.9777e-03,  1.0127e-02, -7.1582e-03,\n           -7.3571e-03, -1.7002e-02,  3.8600e-03,  9.0563e-03,  3.1623e-03,\n           -1.7422e-02,  2.8487e-02,  4.8947e-03,  2.0249e-03,  1.3775e-02,\n            3.1026e-02,  1.2312e-02, -3.4022e-03, -1.9096e-02, -8.7159e-03,\n            1.1083e-02, -9.6974e-04, -2.1345e-02, -1.9912e-02, -1.5728e-02,\n           -4.3275e-03,  1.7562e-03,  3.5679e-03, -2.3053e-03, -2.7260e-03,\n            2.3310e-02,  7.9569e-04,  2.7619e-02, -3.5039e-02,  1.2936e-02,\n           -1.1028e-02, -3.4722e-02,  2.3009e-02, -1.6241e-02, -2.4682e-02,\n           -2.5445e-03,  4.5931e-02, -7.6177e-03, -9.9105e-03, -8.5796e-03,\n           -2.3748e-02, -1.3273e-02, -1.0843e-02, -1.0290e-02,  3.9717e-02,\n            2.4299e-02, -2.0591e-02,  4.1565e-04,  8.7485e-03,  8.0397e-03,\n           -1.9112e-02,  2.5425e-02, -1.5687e-02,  4.2434e-02, -5.1779e-03,\n           -8.2418e-03,  2.3106e-02,  5.5216e-03,  7.4493e-04,  2.0796e-02,\n            1.7484e-02, -5.2373e-03, -2.8117e-02, -2.5178e-02,  2.2788e-02,\n            4.2649e-03,  5.6180e-03, -2.1076e-02, -4.1002e-02, -3.4551e-03,\n           -1.5485e-03, -6.1271e-03, -3.4747e-02,  1.2382e-02,  5.2953e-03,\n            1.7936e-02, -3.5038e-03, -9.1821e-03, -1.5046e-02, -4.4475e-03,\n            7.8396e-03, -1.4451e-02, -1.9457e-03, -6.4955e-03, -8.0623e-03,\n           -5.9862e-02, -9.5166e-03,  2.2533e-02,  3.6839e-02, -6.8352e-03,\n           -5.2056e-03, -2.5660e-04, -1.5027e-02, -2.7513e-02,  1.3943e-02,\n            1.7417e-02,  1.8270e-03, -1.8902e-02, -2.7212e-02,  2.3308e-04,\n           -7.3254e-03, -1.3828e-02, -2.2546e-03,  2.4709e-03,  2.0412e-02,\n           -7.4914e-03,  2.4048e-02,  1.4787e-02, -6.7454e-03, -5.2685e-03,\n            5.3261e-03,  1.9811e-02, -1.5143e-02,  2.6456e-02,  3.3288e-02,\n            1.3041e-02, -5.2443e-02,  2.0290e-02,  1.8272e-02,  1.8988e-02,\n            6.0576e-04, -6.9944e-03, -1.1974e-02,  1.2362e-03, -7.4285e-03,\n           -2.3671e-02, -3.6236e-02, -3.5870e-02,  1.7791e-03,  6.4612e-03,\n            2.5001e-02,  3.2856e-02, -2.2236e-02,  2.7033e-02, -4.0354e-02,\n           -4.2558e-03,  2.3688e-02,  4.2200e-02, -2.3211e-03, -7.0978e-03,\n            2.7538e-02,  1.1015e-02, -1.1061e-02,  1.0797e-02, -2.7064e-02,\n           -1.8449e-02, -1.4359e-03, -2.0756e-02, -1.3274e-02, -1.6928e-02,\n            2.6303e-02,  1.2184e-02,  4.1831e-04, -1.7359e-02, -2.1638e-03,\n            1.9645e-02,  1.3135e-02,  3.2755e-02, -3.1832e-03,  9.4535e-03,\n           -2.8121e-02,  3.3344e-02, -1.3787e-02,  8.5852e-03, -3.4802e-03,\n           -1.3684e-02,  7.3759e-03,  2.1282e-02,  2.4715e-02,  2.9944e-02,\n            1.6015e-02, -2.7024e-02, -1.1709e-02, -1.3275e-02,  1.8891e-03,\n            7.8008e-03,  1.1922e-02,  2.7956e-02,  3.3726e-02,  8.7125e-03,\n            6.2222e-03, -1.3375e-02, -1.2528e-02, -9.1412e-03, -2.0814e-02,\n            3.0376e-02, -2.1553e-02, -2.9182e-04,  3.2498e-03,  1.5922e-02,\n           -6.6553e-03, -1.2650e-02, -3.6693e-03, -1.0224e-03, -5.1008e-03,\n            1.1194e-02,  6.5957e-03, -3.2887e-02,  1.4439e-02,  1.5571e-02,\n            4.0092e-03,  3.7345e-03,  2.9104e-03, -9.3586e-03,  1.3991e-02,\n           -1.8818e-02,  5.3442e-03,  1.1013e-02, -1.8074e-02, -4.2297e-03,\n           -1.7250e-02, -8.8636e-03,  8.5963e-03,  1.5867e-02,  1.1993e-02,\n            2.8774e-02,  9.1964e-03, -1.6134e-02, -1.2745e-03, -1.8641e-02,\n            1.7916e-02,  2.3878e-02, -2.5833e-02, -2.1781e-02,  1.9579e-02,\n           -1.6517e-03, -1.6153e-02,  2.5136e-02,  3.9247e-03,  7.9530e-03,\n            1.7954e-03, -3.0026e-02,  9.6820e-03,  1.6447e-02, -1.8667e-02,\n            1.5215e-02, -3.3251e-03, -1.5632e-02,  1.7843e-02,  9.4584e-04,\n            1.1382e-02, -2.5519e-02, -6.9263e-03, -1.5738e-02, -5.7551e-04,\n            6.2038e-03,  3.0114e-03,  1.8081e-02,  3.0206e-02, -2.4192e-03,\n            6.8965e-04,  1.8176e-02, -5.9115e-02,  5.3141e-03, -1.0247e-02,\n           -4.4376e-02,  3.5316e-03, -4.6654e-03,  2.0014e-03, -4.4579e-03,\n            2.1499e-02, -1.0281e-02, -1.6502e-02,  2.8793e-02,  8.8558e-03,\n            1.2844e-02,  9.5039e-03,  1.7418e-02, -2.2897e-02,  3.6617e-02,\n            1.3011e-02,  7.7224e-03, -7.0324e-03, -2.2982e-02,  4.3579e-02,\n            1.6036e-02,  1.3535e-02,  1.2081e-02, -3.5077e-02,  1.1810e-02,\n            3.3186e-03, -1.3261e-02, -3.6884e-03,  4.3197e-03, -1.9365e-02,\n            1.5614e-02,  4.6674e-02, -4.0866e-02,  1.4834e-02, -2.8548e-02,\n            3.3732e-03,  5.7918e-03,  3.2520e-02,  1.7330e-02, -2.9763e-02,\n           -7.8286e-03, -7.1097e-03,  1.4036e-03,  8.8847e-03,  1.5146e-02,\n           -1.7209e-02,  1.6731e-02, -1.8939e-02, -1.1532e-02, -2.3956e-02,\n           -3.7457e-03, -2.6908e-02,  1.0824e-02,  3.2750e-02, -5.5088e-04,\n           -6.5426e-04, -9.7442e-03, -1.1135e-02,  2.9182e-02, -2.7205e-03,\n            2.4723e-02,  2.4426e-02,  7.1916e-03,  6.3485e-03, -2.8149e-02,\n           -6.2560e-03, -1.5458e-03, -5.2228e-04,  3.6379e-02,  4.1972e-03,\n           -1.6439e-02,  2.0047e-02,  1.1582e-02,  7.0379e-03, -7.4089e-03,\n            1.6721e-02, -2.5651e-02, -2.8596e-02,  7.4818e-03, -1.2831e-02,\n           -1.7649e-02,  1.2692e-02,  3.2324e-03, -1.3114e-02, -9.0881e-03,\n            9.4412e-04, -2.0949e-02,  1.7385e-02, -1.3224e-02,  2.7797e-02,\n            1.4321e-02, -1.1032e-02,  1.9379e-03,  3.0689e-02, -1.7447e-02,\n            7.8940e-03,  6.2562e-03, -3.6796e-02, -1.9966e-02, -2.8952e-02,\n           -2.1302e-02,  4.1724e-03,  3.6348e-02, -1.1241e-02, -2.9566e-02,\n           -4.9880e-03,  1.2801e-02, -9.3037e-03,  4.0803e-02,  4.7651e-02,\n           -6.8860e-03,  2.3072e-02, -2.1191e-02, -3.2276e-02,  3.6749e-02,\n            3.2353e-02,  2.0890e-03, -1.6841e-03, -1.8617e-02,  7.2967e-04,\n            1.3174e-03, -1.3815e-02,  4.7194e-03,  1.4080e-02,  4.7342e-03,\n            7.9995e-04,  2.2473e-02,  1.7314e-02,  2.1295e-02,  1.1221e-02,\n            1.6308e-02, -3.8425e-02, -3.0868e-03, -1.2832e-02,  6.1974e-03,\n           -6.6123e-03,  7.0697e-03, -1.8989e-02,  2.5134e-02,  6.2808e-03,\n            4.3784e-02, -3.5359e-03, -2.2090e-02,  1.5710e-04, -9.8472e-03,\n            2.7536e-03, -8.0779e-03, -6.7901e-03,  4.8863e-03,  5.7348e-02,\n            1.9228e-03, -1.5463e-02,  1.2643e-02, -9.9796e-03, -2.1050e-02,\n            7.8348e-03, -2.9814e-02, -3.2502e-02, -1.7676e-02, -2.3001e-03,\n            5.3991e-03, -6.7847e-03, -3.0278e-04,  1.1877e-02,  1.5142e-03,\n            1.7492e-02,  1.2610e-02, -1.6181e-02, -2.4593e-02,  2.9523e-03,\n            2.1305e-02, -3.3968e-02, -2.4885e-02, -1.6575e-02, -3.4044e-03,\n           -1.8520e-03, -2.6608e-02,  2.8421e-02,  1.6923e-02, -7.3097e-03,\n           -1.0948e-02, -1.2065e-02,  3.4482e-02, -1.3081e-03, -3.5089e-03,\n            3.0075e-02,  2.5231e-03, -2.2160e-02, -1.4121e-02,  3.1378e-02,\n            3.6135e-03, -2.7244e-04, -4.0262e-03, -2.5613e-02,  9.2730e-03,\n           -1.2777e-02, -4.8645e-03,  5.6553e-03, -2.3684e-02,  4.3800e-03,\n            4.2632e-03,  4.5277e-02, -1.4802e-02, -5.5725e-03,  2.0615e-02,\n           -2.3718e-02,  5.6244e-03, -1.3918e-02, -5.0345e-02,  1.3681e-02,\n           -8.6383e-03, -9.0625e-03, -2.6246e-02, -2.4733e-02, -2.5189e-02,\n           -2.7726e-02, -9.3533e-04, -6.2498e-03,  1.1721e-02,  7.2844e-03,\n           -2.3112e-02,  1.0020e-02, -9.7827e-04,  5.8463e-03, -2.6764e-02,\n            1.2785e-02,  2.0553e-02, -5.1940e-03, -6.3866e-03, -1.7984e-02,\n           -4.2112e-02, -6.8824e-04,  2.6915e-02, -1.5712e-02, -7.0578e-03,\n           -2.5767e-02, -9.9187e-03, -1.5535e-02,  1.6702e-02,  2.2838e-02,\n           -4.5030e-03,  3.6509e-02, -1.8748e-02, -2.6405e-02, -3.1211e-04,\n            6.3885e-03,  1.6079e-02,  6.6468e-03,  1.4988e-02,  8.4271e-03,\n            1.9808e-02, -1.0046e-02, -3.0405e-02,  2.9104e-02,  6.5353e-03,\n            1.4716e-03, -6.5926e-04, -1.2818e-02, -1.7999e-02,  2.5630e-03,\n           -1.1687e-02, -3.4722e-03,  1.4360e-02,  1.2885e-02, -2.1128e-02,\n            6.8322e-03, -2.8897e-02,  8.9804e-03,  4.2920e-02,  3.6429e-03,\n            2.2510e-02,  1.3544e-02, -3.2730e-02,  7.9152e-03, -2.2221e-02,\n            1.1949e-02,  2.6879e-02,  4.7669e-02,  1.1706e-02,  4.2149e-03,\n            2.4171e-02, -2.0690e-02,  1.5016e-02,  4.3734e-02,  7.7797e-03,\n           -6.0520e-03, -7.5486e-03,  1.9499e-02, -7.6607e-03,  1.6671e-02,\n            3.4507e-02, -2.1348e-02, -5.4904e-04,  7.7347e-03, -6.8457e-03,\n           -2.2679e-02, -6.5016e-04,  3.4387e-02, -1.8078e-02, -1.2440e-02,\n           -1.2742e-02,  2.1551e-03, -2.4241e-02, -7.0719e-03,  6.3439e-03,\n           -3.4899e-02,  1.9578e-02,  4.4852e-03, -3.3679e-02,  2.4536e-02,\n            4.0598e-02, -3.1164e-03,  9.0473e-03, -3.2285e-03, -7.9785e-04,\n            1.8688e-02,  2.1634e-03,  6.9629e-03, -1.3753e-02, -1.9420e-02,\n            2.3641e-02,  4.0455e-02, -1.5392e-02, -1.5883e-02,  8.4646e-03,\n            1.4248e-02,  2.1196e-02, -1.6404e-02,  2.3822e-02,  1.6771e-02,\n            1.1443e-02, -2.9153e-03,  1.3520e-02, -9.8708e-04,  4.8524e-03,\n            1.8542e-02,  3.6313e-02,  8.2285e-03, -9.0381e-03, -1.0202e-02,\n           -2.9435e-02, -1.0597e-02, -9.3552e-03,  3.1006e-02, -2.8364e-02,\n           -2.2834e-02,  2.6504e-02,  2.5511e-02,  6.8478e-03,  3.2936e-03,\n            1.4241e-02, -2.9134e-03,  5.6734e-03, -8.3642e-03, -2.8988e-02,\n           -2.6532e-02,  2.1771e-02, -4.8590e-03, -1.2596e-03,  2.3239e-02,\n           -1.9154e-03,  4.9132e-04, -1.0398e-02,  2.7894e-02,  1.7985e-02,\n           -1.7061e-02,  8.8470e-04, -2.5208e-02, -1.7584e-02,  2.0064e-03,\n           -1.1719e-02,  8.6139e-03,  5.2935e-03, -7.7774e-03, -2.8559e-02,\n           -2.1339e-03, -2.2030e-02,  3.5696e-02,  2.0784e-02,  3.2141e-02,\n           -1.5820e-02, -8.4556e-03, -1.2251e-02, -2.0556e-02, -2.3004e-03,\n           -4.5438e-02, -3.2701e-03, -1.2330e-02,  1.6534e-02,  3.7389e-03,\n            2.1012e-03, -1.6737e-02,  4.2773e-03, -3.6822e-02, -4.9740e-03,\n            2.7980e-02, -1.4586e-02, -4.8706e-03, -2.2445e-02,  6.5419e-03,\n           -9.0061e-03,  2.7460e-03, -1.4088e-02, -1.2008e-02,  2.5475e-03,\n           -2.4169e-02,  8.1072e-04,  5.8086e-03,  2.9643e-03,  3.7878e-02,\n           -1.2229e-03,  2.4668e-02,  2.4790e-02, -3.2747e-03,  1.1224e-02,\n           -1.2740e-02,  1.1268e-02, -1.1188e-02,  7.3525e-03, -1.4463e-02,\n            2.5714e-02,  4.2423e-03, -1.4912e-02]]], requires_grad=True),\n torch.Size([1, 1, 768]))"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_embeddings.cls_token, vit_embeddings.cls_token.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 1.0774e-02,  7.7184e-03,  5.8833e-03,  8.9845e-03, -3.4352e-02,\n            3.4248e-03,  4.7736e-03,  1.3227e-02,  1.8124e-02, -1.5146e-02,\n           -2.5704e-02,  1.8151e-02, -1.5896e-02, -3.8172e-02,  1.8153e-02,\n           -1.9877e-02, -9.5269e-04, -4.5868e-02, -3.2151e-03,  1.2454e-02,\n           -4.6562e-02, -2.1301e-04, -3.2736e-02, -1.6554e-02, -2.0955e-02,\n           -4.7700e-03,  4.0389e-02,  2.0755e-02,  3.0682e-02,  1.7040e-02,\n            8.8829e-03,  8.8651e-03,  1.8762e-02,  1.7758e-03,  2.7143e-03,\n            1.2448e-02,  5.2147e-02, -1.2137e-02,  3.9249e-03, -5.1513e-02,\n            1.9513e-02,  8.2119e-03,  7.0421e-03, -3.1023e-05, -1.2054e-02,\n            3.2577e-02,  2.4956e-02,  1.1123e-02,  2.2424e-03,  2.4977e-03,\n           -9.4353e-03,  8.4229e-03, -2.2655e-02, -1.4874e-02,  1.3114e-02,\n            1.1049e-02, -1.7435e-02, -3.5369e-02,  2.3251e-03,  1.9796e-02,\n            7.8293e-03,  2.6074e-03, -2.9100e-04, -8.9573e-03,  8.1006e-03,\n           -3.2777e-02,  1.2541e-02,  4.5902e-03, -4.5868e-02, -1.8093e-02,\n           -4.8141e-02,  2.5668e-02,  1.3822e-02, -3.4809e-03, -5.2264e-03,\n            8.5803e-03, -1.3684e-02, -2.1483e-02, -1.1502e-02,  4.4336e-02,\n           -1.4205e-02, -1.0597e-02, -1.2623e-02, -2.0844e-03,  2.9332e-03,\n            6.8332e-03,  6.1783e-03,  7.9777e-03,  1.0127e-02, -7.1582e-03,\n           -7.3571e-03, -1.7002e-02,  3.8600e-03,  9.0563e-03,  3.1623e-03,\n           -1.7422e-02,  2.8487e-02,  4.8947e-03,  2.0249e-03,  1.3775e-02,\n            3.1026e-02,  1.2312e-02, -3.4022e-03, -1.9096e-02, -8.7159e-03,\n            1.1083e-02, -9.6974e-04, -2.1345e-02, -1.9912e-02, -1.5728e-02,\n           -4.3275e-03,  1.7562e-03,  3.5679e-03, -2.3053e-03, -2.7260e-03,\n            2.3310e-02,  7.9569e-04,  2.7619e-02, -3.5039e-02,  1.2936e-02,\n           -1.1028e-02, -3.4722e-02,  2.3009e-02, -1.6241e-02, -2.4682e-02,\n           -2.5445e-03,  4.5931e-02, -7.6177e-03, -9.9105e-03, -8.5796e-03,\n           -2.3748e-02, -1.3273e-02, -1.0843e-02, -1.0290e-02,  3.9717e-02,\n            2.4299e-02, -2.0591e-02,  4.1565e-04,  8.7485e-03,  8.0397e-03,\n           -1.9112e-02,  2.5425e-02, -1.5687e-02,  4.2434e-02, -5.1779e-03,\n           -8.2418e-03,  2.3106e-02,  5.5216e-03,  7.4493e-04,  2.0796e-02,\n            1.7484e-02, -5.2373e-03, -2.8117e-02, -2.5178e-02,  2.2788e-02,\n            4.2649e-03,  5.6180e-03, -2.1076e-02, -4.1002e-02, -3.4551e-03,\n           -1.5485e-03, -6.1271e-03, -3.4747e-02,  1.2382e-02,  5.2953e-03,\n            1.7936e-02, -3.5038e-03, -9.1821e-03, -1.5046e-02, -4.4475e-03,\n            7.8396e-03, -1.4451e-02, -1.9457e-03, -6.4955e-03, -8.0623e-03,\n           -5.9862e-02, -9.5166e-03,  2.2533e-02,  3.6839e-02, -6.8352e-03,\n           -5.2056e-03, -2.5660e-04, -1.5027e-02, -2.7513e-02,  1.3943e-02,\n            1.7417e-02,  1.8270e-03, -1.8902e-02, -2.7212e-02,  2.3308e-04,\n           -7.3254e-03, -1.3828e-02, -2.2546e-03,  2.4709e-03,  2.0412e-02,\n           -7.4914e-03,  2.4048e-02,  1.4787e-02, -6.7454e-03, -5.2685e-03,\n            5.3261e-03,  1.9811e-02, -1.5143e-02,  2.6456e-02,  3.3288e-02,\n            1.3041e-02, -5.2443e-02,  2.0290e-02,  1.8272e-02,  1.8988e-02,\n            6.0576e-04, -6.9944e-03, -1.1974e-02,  1.2362e-03, -7.4285e-03,\n           -2.3671e-02, -3.6236e-02, -3.5870e-02,  1.7791e-03,  6.4612e-03,\n            2.5001e-02,  3.2856e-02, -2.2236e-02,  2.7033e-02, -4.0354e-02,\n           -4.2558e-03,  2.3688e-02,  4.2200e-02, -2.3211e-03, -7.0978e-03,\n            2.7538e-02,  1.1015e-02, -1.1061e-02,  1.0797e-02, -2.7064e-02,\n           -1.8449e-02, -1.4359e-03, -2.0756e-02, -1.3274e-02, -1.6928e-02,\n            2.6303e-02,  1.2184e-02,  4.1831e-04, -1.7359e-02, -2.1638e-03,\n            1.9645e-02,  1.3135e-02,  3.2755e-02, -3.1832e-03,  9.4535e-03,\n           -2.8121e-02,  3.3344e-02, -1.3787e-02,  8.5852e-03, -3.4802e-03,\n           -1.3684e-02,  7.3759e-03,  2.1282e-02,  2.4715e-02,  2.9944e-02,\n            1.6015e-02, -2.7024e-02, -1.1709e-02, -1.3275e-02,  1.8891e-03,\n            7.8008e-03,  1.1922e-02,  2.7956e-02,  3.3726e-02,  8.7125e-03,\n            6.2222e-03, -1.3375e-02, -1.2528e-02, -9.1412e-03, -2.0814e-02,\n            3.0376e-02, -2.1553e-02, -2.9182e-04,  3.2498e-03,  1.5922e-02,\n           -6.6553e-03, -1.2650e-02, -3.6693e-03, -1.0224e-03, -5.1008e-03,\n            1.1194e-02,  6.5957e-03, -3.2887e-02,  1.4439e-02,  1.5571e-02,\n            4.0092e-03,  3.7345e-03,  2.9104e-03, -9.3586e-03,  1.3991e-02,\n           -1.8818e-02,  5.3442e-03,  1.1013e-02, -1.8074e-02, -4.2297e-03,\n           -1.7250e-02, -8.8636e-03,  8.5963e-03,  1.5867e-02,  1.1993e-02,\n            2.8774e-02,  9.1964e-03, -1.6134e-02, -1.2745e-03, -1.8641e-02,\n            1.7916e-02,  2.3878e-02, -2.5833e-02, -2.1781e-02,  1.9579e-02,\n           -1.6517e-03, -1.6153e-02,  2.5136e-02,  3.9247e-03,  7.9530e-03,\n            1.7954e-03, -3.0026e-02,  9.6820e-03,  1.6447e-02, -1.8667e-02,\n            1.5215e-02, -3.3251e-03, -1.5632e-02,  1.7843e-02,  9.4584e-04,\n            1.1382e-02, -2.5519e-02, -6.9263e-03, -1.5738e-02, -5.7551e-04,\n            6.2038e-03,  3.0114e-03,  1.8081e-02,  3.0206e-02, -2.4192e-03,\n            6.8965e-04,  1.8176e-02, -5.9115e-02,  5.3141e-03, -1.0247e-02,\n           -4.4376e-02,  3.5316e-03, -4.6654e-03,  2.0014e-03, -4.4579e-03,\n            2.1499e-02, -1.0281e-02, -1.6502e-02,  2.8793e-02,  8.8558e-03,\n            1.2844e-02,  9.5039e-03,  1.7418e-02, -2.2897e-02,  3.6617e-02,\n            1.3011e-02,  7.7224e-03, -7.0324e-03, -2.2982e-02,  4.3579e-02,\n            1.6036e-02,  1.3535e-02,  1.2081e-02, -3.5077e-02,  1.1810e-02,\n            3.3186e-03, -1.3261e-02, -3.6884e-03,  4.3197e-03, -1.9365e-02,\n            1.5614e-02,  4.6674e-02, -4.0866e-02,  1.4834e-02, -2.8548e-02,\n            3.3732e-03,  5.7918e-03,  3.2520e-02,  1.7330e-02, -2.9763e-02,\n           -7.8286e-03, -7.1097e-03,  1.4036e-03,  8.8847e-03,  1.5146e-02,\n           -1.7209e-02,  1.6731e-02, -1.8939e-02, -1.1532e-02, -2.3956e-02,\n           -3.7457e-03, -2.6908e-02,  1.0824e-02,  3.2750e-02, -5.5088e-04,\n           -6.5426e-04, -9.7442e-03, -1.1135e-02,  2.9182e-02, -2.7205e-03,\n            2.4723e-02,  2.4426e-02,  7.1916e-03,  6.3485e-03, -2.8149e-02,\n           -6.2560e-03, -1.5458e-03, -5.2228e-04,  3.6379e-02,  4.1972e-03,\n           -1.6439e-02,  2.0047e-02,  1.1582e-02,  7.0379e-03, -7.4089e-03,\n            1.6721e-02, -2.5651e-02, -2.8596e-02,  7.4818e-03, -1.2831e-02,\n           -1.7649e-02,  1.2692e-02,  3.2324e-03, -1.3114e-02, -9.0881e-03,\n            9.4412e-04, -2.0949e-02,  1.7385e-02, -1.3224e-02,  2.7797e-02,\n            1.4321e-02, -1.1032e-02,  1.9379e-03,  3.0689e-02, -1.7447e-02,\n            7.8940e-03,  6.2562e-03, -3.6796e-02, -1.9966e-02, -2.8952e-02,\n           -2.1302e-02,  4.1724e-03,  3.6348e-02, -1.1241e-02, -2.9566e-02,\n           -4.9880e-03,  1.2801e-02, -9.3037e-03,  4.0803e-02,  4.7651e-02,\n           -6.8860e-03,  2.3072e-02, -2.1191e-02, -3.2276e-02,  3.6749e-02,\n            3.2353e-02,  2.0890e-03, -1.6841e-03, -1.8617e-02,  7.2967e-04,\n            1.3174e-03, -1.3815e-02,  4.7194e-03,  1.4080e-02,  4.7342e-03,\n            7.9995e-04,  2.2473e-02,  1.7314e-02,  2.1295e-02,  1.1221e-02,\n            1.6308e-02, -3.8425e-02, -3.0868e-03, -1.2832e-02,  6.1974e-03,\n           -6.6123e-03,  7.0697e-03, -1.8989e-02,  2.5134e-02,  6.2808e-03,\n            4.3784e-02, -3.5359e-03, -2.2090e-02,  1.5710e-04, -9.8472e-03,\n            2.7536e-03, -8.0779e-03, -6.7901e-03,  4.8863e-03,  5.7348e-02,\n            1.9228e-03, -1.5463e-02,  1.2643e-02, -9.9796e-03, -2.1050e-02,\n            7.8348e-03, -2.9814e-02, -3.2502e-02, -1.7676e-02, -2.3001e-03,\n            5.3991e-03, -6.7847e-03, -3.0278e-04,  1.1877e-02,  1.5142e-03,\n            1.7492e-02,  1.2610e-02, -1.6181e-02, -2.4593e-02,  2.9523e-03,\n            2.1305e-02, -3.3968e-02, -2.4885e-02, -1.6575e-02, -3.4044e-03,\n           -1.8520e-03, -2.6608e-02,  2.8421e-02,  1.6923e-02, -7.3097e-03,\n           -1.0948e-02, -1.2065e-02,  3.4482e-02, -1.3081e-03, -3.5089e-03,\n            3.0075e-02,  2.5231e-03, -2.2160e-02, -1.4121e-02,  3.1378e-02,\n            3.6135e-03, -2.7244e-04, -4.0262e-03, -2.5613e-02,  9.2730e-03,\n           -1.2777e-02, -4.8645e-03,  5.6553e-03, -2.3684e-02,  4.3800e-03,\n            4.2632e-03,  4.5277e-02, -1.4802e-02, -5.5725e-03,  2.0615e-02,\n           -2.3718e-02,  5.6244e-03, -1.3918e-02, -5.0345e-02,  1.3681e-02,\n           -8.6383e-03, -9.0625e-03, -2.6246e-02, -2.4733e-02, -2.5189e-02,\n           -2.7726e-02, -9.3533e-04, -6.2498e-03,  1.1721e-02,  7.2844e-03,\n           -2.3112e-02,  1.0020e-02, -9.7827e-04,  5.8463e-03, -2.6764e-02,\n            1.2785e-02,  2.0553e-02, -5.1940e-03, -6.3866e-03, -1.7984e-02,\n           -4.2112e-02, -6.8824e-04,  2.6915e-02, -1.5712e-02, -7.0578e-03,\n           -2.5767e-02, -9.9187e-03, -1.5535e-02,  1.6702e-02,  2.2838e-02,\n           -4.5030e-03,  3.6509e-02, -1.8748e-02, -2.6405e-02, -3.1211e-04,\n            6.3885e-03,  1.6079e-02,  6.6468e-03,  1.4988e-02,  8.4271e-03,\n            1.9808e-02, -1.0046e-02, -3.0405e-02,  2.9104e-02,  6.5353e-03,\n            1.4716e-03, -6.5926e-04, -1.2818e-02, -1.7999e-02,  2.5630e-03,\n           -1.1687e-02, -3.4722e-03,  1.4360e-02,  1.2885e-02, -2.1128e-02,\n            6.8322e-03, -2.8897e-02,  8.9804e-03,  4.2920e-02,  3.6429e-03,\n            2.2510e-02,  1.3544e-02, -3.2730e-02,  7.9152e-03, -2.2221e-02,\n            1.1949e-02,  2.6879e-02,  4.7669e-02,  1.1706e-02,  4.2149e-03,\n            2.4171e-02, -2.0690e-02,  1.5016e-02,  4.3734e-02,  7.7797e-03,\n           -6.0520e-03, -7.5486e-03,  1.9499e-02, -7.6607e-03,  1.6671e-02,\n            3.4507e-02, -2.1348e-02, -5.4904e-04,  7.7347e-03, -6.8457e-03,\n           -2.2679e-02, -6.5016e-04,  3.4387e-02, -1.8078e-02, -1.2440e-02,\n           -1.2742e-02,  2.1551e-03, -2.4241e-02, -7.0719e-03,  6.3439e-03,\n           -3.4899e-02,  1.9578e-02,  4.4852e-03, -3.3679e-02,  2.4536e-02,\n            4.0598e-02, -3.1164e-03,  9.0473e-03, -3.2285e-03, -7.9785e-04,\n            1.8688e-02,  2.1634e-03,  6.9629e-03, -1.3753e-02, -1.9420e-02,\n            2.3641e-02,  4.0455e-02, -1.5392e-02, -1.5883e-02,  8.4646e-03,\n            1.4248e-02,  2.1196e-02, -1.6404e-02,  2.3822e-02,  1.6771e-02,\n            1.1443e-02, -2.9153e-03,  1.3520e-02, -9.8708e-04,  4.8524e-03,\n            1.8542e-02,  3.6313e-02,  8.2285e-03, -9.0381e-03, -1.0202e-02,\n           -2.9435e-02, -1.0597e-02, -9.3552e-03,  3.1006e-02, -2.8364e-02,\n           -2.2834e-02,  2.6504e-02,  2.5511e-02,  6.8478e-03,  3.2936e-03,\n            1.4241e-02, -2.9134e-03,  5.6734e-03, -8.3642e-03, -2.8988e-02,\n           -2.6532e-02,  2.1771e-02, -4.8590e-03, -1.2596e-03,  2.3239e-02,\n           -1.9154e-03,  4.9132e-04, -1.0398e-02,  2.7894e-02,  1.7985e-02,\n           -1.7061e-02,  8.8470e-04, -2.5208e-02, -1.7584e-02,  2.0064e-03,\n           -1.1719e-02,  8.6139e-03,  5.2935e-03, -7.7774e-03, -2.8559e-02,\n           -2.1339e-03, -2.2030e-02,  3.5696e-02,  2.0784e-02,  3.2141e-02,\n           -1.5820e-02, -8.4556e-03, -1.2251e-02, -2.0556e-02, -2.3004e-03,\n           -4.5438e-02, -3.2701e-03, -1.2330e-02,  1.6534e-02,  3.7389e-03,\n            2.1012e-03, -1.6737e-02,  4.2773e-03, -3.6822e-02, -4.9740e-03,\n            2.7980e-02, -1.4586e-02, -4.8706e-03, -2.2445e-02,  6.5419e-03,\n           -9.0061e-03,  2.7460e-03, -1.4088e-02, -1.2008e-02,  2.5475e-03,\n           -2.4169e-02,  8.1072e-04,  5.8086e-03,  2.9643e-03,  3.7878e-02,\n           -1.2229e-03,  2.4668e-02,  2.4790e-02, -3.2747e-03,  1.1224e-02,\n           -1.2740e-02,  1.1268e-02, -1.1188e-02,  7.3525e-03, -1.4463e-02,\n            2.5714e-02,  4.2423e-03, -1.4912e-02]]], grad_fn=<AddBackward0>),\n torch.Size([1, 1, 768]))"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token_pos = vit_embeddings.cls_token + vit_embeddings.position_embeddings[:, :1, :]\n",
    "cls_token_pos, cls_token_pos.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.0108,  0.0077,  0.0059,  ...,  0.0257,  0.0042, -0.0149]],\n \n         [[ 0.0108,  0.0077,  0.0059,  ...,  0.0257,  0.0042, -0.0149]]],\n        grad_fn=<ExpandBackward0>),\n torch.Size([2, 1, 768]))"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens = cls_token_pos.expand(embeddings.shape[0], -1, -1)\n",
    "cls_tokens, cls_tokens.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.0108,  0.0077,  0.0059,  ...,  0.0257,  0.0042, -0.0149],\n          [ 0.5278,  0.2685, -0.6473,  ...,  0.8113,  1.0937,  0.7003],\n          [-0.5090, -0.2033, -0.3931,  ...,  0.9461,  1.2617,  0.8079],\n          ...,\n          [ 0.7222, -0.2479, -0.4524,  ...,  0.8263,  1.2958,  0.6242],\n          [-0.7043, -0.5573, -0.9992,  ...,  0.7991,  0.9773,  0.7021],\n          [-0.7664, -0.6398, -0.3093,  ...,  0.9467,  1.2165,  0.4633]],\n \n         [[ 0.0108,  0.0077,  0.0059,  ...,  0.0257,  0.0042, -0.0149],\n          [-0.2689, -0.0183, -0.5378,  ...,  0.6198,  1.1503,  0.7082],\n          [-0.4604, -0.2877, -0.8647,  ...,  0.8965,  0.9679,  0.4046],\n          ...,\n          [-0.6682, -0.2119, -0.2083,  ...,  0.6230,  1.5158,  0.8301],\n          [-0.0754,  0.2375,  0.7344,  ...,  0.9857,  1.5763,  0.4417],\n          [ 0.5742,  1.3351,  1.1841,  ...,  1.1348,  1.2267,  0.6618]]],\n        grad_fn=<CatBackward0>),\n torch.Size([2, 50, 768]))"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "embeddings, embeddings.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
